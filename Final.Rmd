---
title: "Tourism Motivation - 18 Categories"
author: "Llorenç Femenias"
date: "21/11/2019"
output: html_document
---

```{r global_options, include = FALSE, warning = FALSE}
knitr::opts_chunk$set(fig.width=5, fig.height=5,  fig.align="center", echo=TRUE, warning=FALSE, message=FALSE) 
rm(list=ls())
cat("\014") #limpia pantalla
library(tidyverse)
library(gbm)
library(MASS)
library(caTools)
library(caret)
library(ggvis)
library(class)
library(gmodels)
library(ggplot2)
library(foreign)
library(nnet)
library(tree)
library(randomForest)
```

```{r Carga de datos}
setwd("~/Dropbox/MADM/Aprendizaje Estadistico/Final")
df <- read_csv("data_turismo.csv")
df$provdest <- as.factor(df$provdest)
df$transprin <- as.factor(df$transprin)
df$ccaa_residencia <- as.factor(df$ccaa_residencia)
df$motiv <- as.factor(df$motiv)
```

```{r Limpieza}
# Solo año 2017
df %>%
        filter( anyo == 2017) -> df

# Descartar aquellas columnas con más de un 10% de NA's
df <- df[, -which(colMeans(is.na(df)) > 0.1)]

# Eliminamos variables que no son de interes o están repetidas
df <- df[,-c(100)] # columna sobrante
df$paisdest <- NULL # no variación
df$destesp <- NULL # no variación
df$id <- NULL
df$anyo <- NULL
df$idsec <- NULL
df$bloque_etapas <- NULL
df$npernoc <- NULL
df$paquete <- NULL
df$impu_parcial <- NULL
df$gastofi_paq <- NULL
df$gasto_trans <- NULL
df$gasto_barest <- NULL
df$gasto_act <- NULL
df$gasto_aloja <- NULL
df$gasto_biendur <- NULL
df$gasto_resto <- NULL
df$igastofi_paq <- NULL
df$igastofi_act <- NULL
df$igastofi_aloja <- NULL
df$igastofi_barest <- NULL
df$igastofi_biendur <- NULL
df$igastofi_resto <- NULL
df$igastofi_total <- NULL
df$igastofi_trans <- NULL
df$factorgas_tot <- NULL
df$factorvi_15mas <- NULL
df$factorvi_tot <- NULL
df$miemv_15menos <- NULL
df$dest_costa <- NULL
df$dest_pen <- NULL
df$orden_rep <- NULL
df$orden_viaje <- NULL
df$ccaadest <- NULL
df$paisdest <- NULL
df$destesp <- NULL
df$gasto_paq <- NULL
df$gastofi_act <- NULL
df$gastofi_aloja <- NULL
df$gastofi_barest <- NULL
df$gastofi_biendur <- NULL
df$gastofi_resto <- NULL
df$gastofi_trans <- NULL
df$corr15_vt <- NULL
df$transprin <- NULL

# Eliminar NA's
df <- na.omit(df)

# Matriz de correlaciones
df.numeric <- data.matrix(df)
df.numeric <- na.omit(df.numeric)
```


```{r Reducir número de categorías}
df %>%
        filter(motiv != "Visita familiar") %>%
        filter(motiv != "Personal_Otros") %>%
        filter(motiv != "Centro trabajo") %>%
        filter(motiv != "Incentivos") %>%
        filter(motiv != "Salud") %>% 
        filter(motiv != "Congresos") %>% 
        filter(motiv != "Educacion") %>%
        filter(motiv != "Religioso") %>%
        filter(motiv != "Reuniones") -> df


df$motiv <- factor(df$motiv)
df$alojaprin <- factor(df$alojaprin)
```

```{r Distribución observaciones}
df %>%
        group_by(motiv) %>%
        summarise(no_rows = (length(motiv)/nrow(df))*100)
```

```{r Train & Test Sample (Catools)}
set.seed(111)
library(caTools)
split = sample.split(df$motiv, SplitRatio = 0.8)
df.train = subset(df, split == TRUE)
df.test = subset(df, split == FALSE)
```


```{r PCA}
set.seed(111)
pr.out = prcomp(df.numeric, scale=TRUE)
pr.var=pr.out$sdev^2
pve=pr.var/sum(pr.var)
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')
pc.comp <- pr.out$scores
pc.comp1 <- -1*pc.comp[,1]
pc.comp2 <- -1*pc.comp[,2]
```


```{r KNN: CV}
set.seed(111)
cv10 <- trainControl(method  = "cv", number  = 10)
knn.cv10 <- train(motiv ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:10),
             trControl  = cv10,
             metric     = "Accuracy",
             data       = df.train)
knn.cv10
```

```{r Best KNN: K=1}
set.seed(111)
ind <- sample(2, nrow(df.numeric), replace=TRUE, prob=c(0.8, 0.2))
df.train.num <- df.numeric[ind==1, -c(3), drop = TRUE]
df.test.num <- df.numeric[ind==2, -c(3), drop = TRUE]
trainlabes <- df.numeric[ind==1, 3, drop = TRUE]
testlabels <- df.numeric[ind==2, 3, drop = TRUE]

knn.pred.1 <- knn(train = df.train.num, test = df.test.num, cl = trainlabes, k=1)

knn.accuracy.k1 = mean(knn.pred.1==testlabels)
knn.accuracy.k1
```

```{r Multinomial Logit CV10}
set.seed(111)
glm.fit.cv10 <- multinom(motiv ~., data = df.train, MaxNWts = 100000, K = 10, maxit = 1000)
glm.preds.cv10 = predict(glm.fit.cv10, type="class", newdata = df.test)
logit.accuracy = mean(glm.preds.cv10==df.test$motiv)
logit.accuracy
```

```{r Linear Discriminant Analysis}
lda.fit <- train(motiv ~ . -provdest,
             method     = "lda",
             trControl  = cv10,
             metric     = "Accuracy",
             data       = df.train)
lda.pred = predict(lda.fit, newdata = df.test)
table(lda.pred, df.test$motiv)
lda.accuracy = mean(lda.pred == df.test$motiv)
lda.accuracy
```

```{r Decision Tree}
set.seed(111)
tree.tourism = tree(motiv~. -provdest, data = df.train)
summary(tree.tourism)
plot(tree.tourism); text(tree.tourism, pretty = 0)
tree.pred = predict(tree.tourism, df.test, type = "class")
with(df.test, table(tree.pred, motiv))
tree.accuracy = mean(tree.pred==df.test$motiv)
tree.accuracy
```

```{r Decision Tree: CV to prune}
tree.cv.tourism = cv.tree(tree.tourism, FUN= prune.misclass)
plot(tree.cv.tourism)
prune.tourism = prune.misclass(tree.tourism, best = 5)
plot(prune.tourism); text(prune.tourism, pretty = 0)
tree.pred.cv = predict(prune.tourism, df.test, type="class")
with(df.test, table(tree.pred.cv, motiv))
tree.accuracy.cv = mean(tree.pred.cv==df.test$motiv)
tree.accuracy.cv
```

```{r Random Forest}
set.seed(111)
# 60% Training, 20% Validation, 20% Test 
inTraining <- createDataPartition(df.train$motiv, p=0.75, list=FALSE)
training.set <- df.train[inTraining,]
validation.set <- df.train[-inTraining,]

dataset <- training.set
validation <- validation.set
test <- df.test
# The final model will actually use all data, except test
total <- rbind(dataset, validation)

# Reduce the grid to save time here. 
fitControl <- trainControl(method = 'cv', number = 5, summaryFunction=defaultSummary)

rfGrid <-  expand.grid(mtry = c(1,3, 5,7,9))

### Random Forest algorithm. ###
fit.rf <- train(motiv~., data=dataset, method = 'rf', trControl=fitControl, tuneGrid=rfGrid, metric='Accuracy', distribution='multinomial')
fit.rf
plot(fit.rf)
res_rf <- fit.rf$results
acc_rf <- subset(res_rf[2])
# CV con mejor "tune"
max(acc_rf)

rf.caret.pred <- predict(fit.rf,validation)

table(rf.caret.pred ,validation$motiv)
mean(rf.caret.pred==validation$motiv)

fit.rf_total <- train(motiv~., data=total, method = 'rf', trControl=fitControl, tuneGrid=rfGrid, metric='Accuracy', distribution='multinomial')
fit.rf_total
plot(fit.rf_total)
res_rf_total <- fit.rf_total$results
acc_rf_total <- subset(res_rf_total[2]) 
# CV con mejor "tune" 
max(acc_rf_total)
#Evaluate on test
rf.caret.pred_total <- predict(fit.rf_total,test)

table(rf.caret.pred_total ,test$motiv)
rf.accuracy = mean(rf.caret.pred_total==test$motiv)
```



```{r Boosting CV}
set.seed(111)
#PREPARE A GRID. NOTE: All combinations will be evaluated!
gbmGrid <-  expand.grid(interaction.depth = c(2,4,6),
                        n.trees = c(100, 200, 500),
                        shrinkage = c(.001, .01),
                        n.minobsinnode = 10)

fitControl <- trainControl(method = 'cv', number = 5, summaryFunction=defaultSummary)

### Gradient boosting machine algorithm. ###
fit.gbm <- train(motiv~., data=dataset, method = 'gbm', trControl=fitControl, tuneGrid=gbmGrid, metric='Accuracy', distribution='multinomial')
fit.gbm
plot(fit.gbm)
fit.gbm$bestTune

res_gbm <- fit.gbm$results
acc_gbm <- subset(res_gbm[5])
# CV con mejor "tune"
max(acc_gbm)

boost.caret.pred <- predict(fit.gbm,validation)
table(boost.caret.pred ,validation$motiv)
mean(boost.caret.pred==validation$motiv)

fit.gbm_total <- train(motiv~., data=total, method = 'gbm', trControl=fitControl, tuneGrid=gbmGrid, metric='Accuracy', distribution='multinomial')
fit.gbm_total
plot(fit.gbm_total)
fit.gbm_total$bestTune

res_gbm_total <- fit.gbm_total$results
acc_gbm_total <- subset(res_gbm_total[5])
# CV con mejor "tune"
max(acc_gbm_total)

#Evaluate on test
boost.caret.pred_total <- predict(fit.gbm_total,test)

table(boost.caret.pred_total ,test$motiv)
bgm.accuracy = mean(boost.caret.pred_total==test$motiv)
summary(fit.gbm_total, n.trees=500)
```

```{r Resumen}
table(knn.accuracy.k1, logit.accuracy, lda.accuracy, tree.accuracy, tree.accuracy.cv, rf.accuracy, bgm.accuracy)
```

